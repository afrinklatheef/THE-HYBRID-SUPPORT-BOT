# THE-HYBRID-SUPPORT-BOT
This project builds a Retrieval-Augmented Generation (RAG) assistant that answers questions from a multi-section PDF manual. It extracts chapter metadata at ingest time and uses that metadata to restrict retrieval for scoped queries (hybrid search). The system prints retrieval vs generation latency and returns “I don't know” when evidence is wea
# -*- coding: utf-8 -*-
"""THE "HYBRID" SUPPORT BOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10rbD41j94YrRTGcqt91mR0w0MYvYg7Su

THE HYBRID SUPPORT BOT
"""

!pip install -q pdfminer.six sentence-transformers transformers torch tqdm numpy

from google.colab import files
uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile ingest.py
# """
# ingest.py
# Clean ingestion pipeline for Hybrid RAG.
# 
# What it does:
# 1. Load a multi-section PDF.
# 2. Extract text page-by-page.
# 3. Detect chapter/section headings using simple rules.
# 4. Split each page into chunks with metadata.
# 5. Create embeddings using SentenceTransformers.
# 6. Save:
#    - embeddings.npy   (vector store)
#    - chunks.json      (text + metadata)
# """
# 
# import os
# import json
# import time
# import re
# import numpy as np
# from typing import List, Dict
# from pdfminer.high_level import extract_text
# from sentence_transformers import SentenceTransformer
# 
# # ===== CONFIG =====
# PDF_PATH = "manual.pdf"
# OUTPUT_DIR = "ingest_output"
# CHUNK_SIZE = 800                # characters per chunk
# EMBED_MODEL = "all-MiniLM-L6-v2"
# 
# os.makedirs(OUTPUT_DIR, exist_ok=True)
# 
# 
# def extract_pages(pdf_path: str) -> List[str]:
#     text = extract_text(pdf_path)
#     pages = text.split("\f")
#     return [p.strip() for p in pages if p.strip()]
# 
# 
# def detect_chapter(page_text: str) -> str:
#     """
#     Simple rules:
#     - If the first few lines contain 'Chapter' or 'Section'
#     - Otherwise return 'Unknown'
#     """
#     lines = page_text.splitlines()[:6]
#     for ln in lines:
#         if re.search(r"(chapter|section)\s+\d+", ln, re.IGNORECASE):
#             return ln.strip()
#     for ln in lines:
#         if ln.isupper() and len(ln) < 50:
#             return ln.strip()
#     return "Unknown"
# 
# 
# def chunk_page(text: str, page_number: int, chapter: str) -> List[Dict]:
#     chunks = []
#     for i in range(0, len(text), CHUNK_SIZE):
#         chunk_text = text[i:i + CHUNK_SIZE].strip()
#         if chunk_text:
#             chunks.append({
#                 "text": chunk_text,
#                 "metadata": {
#                     "source": PDF_PATH,
#                     "chapter": chapter,
#                     "page": page_number
#                 }
#             })
#     return chunks
# 
# 
# def main():
#     print("Starting ingestion...")
#     t0 = time.time()
# 
#     pages = extract_pages(PDF_PATH)
#     print(f"Found {len(pages)} pages")
# 
#     all_chunks = []
# 
#     for i, page_text in enumerate(pages, start=1):
#         chapter = detect_chapter(page_text)
#         chunks = chunk_page(page_text, i, chapter)
#         all_chunks.extend(chunks)
# 
#     print(f"Created {len(all_chunks)} text chunks")
# 
#     # Embeddings
#     model = SentenceTransformer(EMBED_MODEL)
#     texts = [c["text"] for c in all_chunks]
#     embeddings = model.encode(texts, convert_to_numpy=True)
#     embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
# 
#     # Save
#     np.save(os.path.join(OUTPUT_DIR, "embeddings.npy"), embeddings)
#     with open(os.path.join(OUTPUT_DIR, "chunks.json"), "w") as f:
#         json.dump(all_chunks, f, indent=2)
# 
#     print("DONE!")
#     print(f"Total time: {time.time() - t0:.2f} sec")
# 
# 
# if __name__ == "__main__":
#     main()
#

!python ingest.py

import json
with open("ingest_output/chunks.json") as f:
    data = json.load(f)

for c in data[:5]:
    print(c["metadata"])
    print(c["text"][:300], "\n\n")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile query_runtime.py
# """
# query_runtime.py
# Clean hybrid retrieval system.
# 
# What it does:
# 1. Loads vector index & metadata.
# 2. Embeds the user query.
# 3. Detects the best matching chapter.
# 4. Filters retrieval to that chapter (if confident).
# 5. Retrieves top-k chunks.
# 6. Generates answer using HF small model.
# 7. Returns 'I don't know' if evidence weak.
# 8. Logs retrieval + generation latency.
# """
# 
# import json
# import time
# import numpy as np
# from sentence_transformers import SentenceTransformer
# from transformers import pipeline
# 
# # ===== CONFIG =====
# INGEST_DIR = "ingest_output"
# EMBED_MODEL = "all-MiniLM-L6-v2"
# GEN_MODEL = "google/flan-t5-small"
# TOP_K = 5
# CHAPTER_THRESHOLD = 0.50   # how confident we must be to filter
# 
# # Cached models
# embed_model = None
# generator = None
# 
# 
# def load_data():
#     embeddings = np.load(f"{INGEST_DIR}/embeddings.npy")
#     with open(f"{INGEST_DIR}/chunks.json") as f:
#         chunks = json.load(f)
#     return embeddings, chunks
# 
# 
# def build_chapter_index(chunks, embeddings):
#     chapters = {}
#     for idx, c in enumerate(chunks):
#         chap = c["metadata"]["chapter"]
#         chapters.setdefault(chap, []).append(idx)
# 
#     chap_names = []
#     chap_embs = []
# 
#     for chap, idxs in chapters.items():
#         mean_emb = embeddings[idxs].mean(axis=0)
#         mean_emb = mean_emb / np.linalg.norm(mean_emb)
#         chap_names.append(chap)
#         chap_embs.append(mean_emb)
# 
#     return chap_names, np.array(chap_embs), chapters
# 
# 
# def answer_query(question: str):
#     global embed_model, generator
# 
#     # Load & cache models
#     if embed_model is None:
#         embed_model = SentenceTransformer(EMBED_MODEL)
# 
#     if generator is None:
#         generator = pipeline("text2text-generation", GEN_MODEL, max_length=300)
# 
#     embeddings, chunks = load_data()
#     chap_names, chap_embs, chap_map = build_chapter_index(chunks, embeddings)
# 
#     # ===== Step 1: embed query =====
#     t0 = time.time()
#     q_emb = embed_model.encode([question])[0]
#     q_emb = q_emb / np.linalg.norm(q_emb)
# 
#     # ===== Step 2: chapter detection =====
#     sims = chap_embs @ q_emb
#     best_idx = int(np.argmax(sims))
#     best_score = float(sims[best_idx])
#     best_chap = chap_names[best_idx]
# 
#     use_filter = best_score >= CHAPTER_THRESHOLD and best_chap != "Unknown"
# 
#     # ===== Step 3: retrieve chunks =====
#     if use_filter:
#         candidate_ids = chap_map[best_chap]
#     else:
#         candidate_ids = range(len(chunks))
# 
#     # cosine similarity
#     cand_embs = embeddings[list(candidate_ids)]
#     scores = cand_embs @ q_emb
#     top_ids = np.argsort(-scores)[:TOP_K]
# 
#     retrieved = [(list(candidate_ids)[i], scores[i]) for i in top_ids]
#     retrieval_latency = time.time() - t0
# 
#     # ===== If low scores, say I DON'T KNOW =====
#     if max(scores) < 0.20:
#         return {
#             "answer": "I don't know — the manual does not provide this information.",
#             "retrieval_latency": retrieval_latency,
#             "generation_latency": 0
#         }
# 
#     # ===== Step 4: context =====
#     context_text = "\n---\n".join([chunks[i]["text"] for i, _ in retrieved])
# 
#     # ===== Step 5: generate =====
#     t1 = time.time()
#     prompt = f"Context:\n{context_text}\n\nQuestion: {question}\nAnswer:"
#     output = generator(prompt)[0]["generated_text"]
#     gen_latency = time.time() - t1
# 
#     # ===== final =====
#     return {
#         "answer": output,
#         "retrieval_latency": retrieval_latency,
#         "generation_latency": gen_latency,
#         "chapter_used": best_chap if use_filter else "None (unfiltered)"
#     }
# 
#

import importlib, sys

spec = importlib.util.spec_from_file_location("query_runtime", "query_runtime.py")
qr = importlib.util.module_from_spec(spec)
sys.modules["query_runtime"] = qr
spec.loader.exec_module(qr)

qr.answer_query("How do I reset configuration settings on the Raspberry Pi?")

qr.answer_query("What are the system requirements?")

qr.answer_query("How do I change the serial number?")
